---
type: insight
title: The core difference between jailbreaking and prompt injection attacks
concepts:
  - "jailbreaking"
  - "prompt-injection"
  - "ai-security"
source_guest: "Sander Schulhoff"
source_episode: "Why securing AI is harder than anyone expected and guardrails are failing | HackAPrompt CEO"
source: "[[guests/sander-schulhoff-20|Sander Schulhoff 2.0]]"
---
[[concepts/jailbreaking|Jailbreaking]] occurs when a user directly tricks a base model (like ChatGPT) into doing something it shouldn't, with no intermediary system prompt. [[concepts/prompt-injection|Prompt injection]] specifically targets applications built on top of models, where an attacker tries to override a developer's system instructions. The latter is more common in real-world deployments where AI is integrated into products.
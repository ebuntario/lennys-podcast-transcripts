---
type: insight
title: Human expert evaluation is superior to automated benchmarks for measuring AI progress
concepts:
  - "human-evaluation"
  - "ai-evaluation"
  - "ai-training"
source_guest: Edwin Chen
source_episode: The $1B Al company training ChatGPT, Claude & Gemini on the path to responsible AGI | Edwin Chen
source: "[[guests/edwin-chen|Edwin Chen]]"
---
The best way to measure AI model progress is through deep [[concepts/human-evaluation|human evaluation]] by domain experts, not automated benchmarks. Experts like Nobel Prize-winning physicists or senior engineers can have extended conversations with a model, deeply evaluate its outputs for accuracy and nuance, and provide a far more reliable assessment of its capabilities on real-world tasks than any standardized test.
---
type: insight
title: The infinite attack space makes perfect AI security impossible
concepts:
  - "adversarial-robustness"
  - "ai-security"
  - "prompt-injection"
source_guest: "Sander Schulhoff"
source_episode: "Why securing AI is harder than anyone expected and guardrails are failing | HackAPrompt CEO"
source: "[[guests/sander-schulhoff-20|Sander Schulhoff 2.0]]"
---
The number of possible adversarial prompts against a model like GPT-5 is astronomically large (on the order of 10^1,000,000), creating a practically infinite attack surface. This makes it impossible to claim a defense catches "everything" or even a statistically significant portion of attacks, fundamentally limiting [[concepts/adversarial-robustness|adversarial robustness]].
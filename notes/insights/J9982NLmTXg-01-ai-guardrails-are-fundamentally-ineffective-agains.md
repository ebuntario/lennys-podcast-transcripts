---
type: insight
title: AI guardrails are fundamentally ineffective against determined attackers
concepts:
  - "ai-security"
  - "adversarial-robustness"
  - "prompt-injection"
  - "jailbreaking"
source_guest: "Sander Schulhoff"
source_episode: "Why securing AI is harder than anyone expected and guardrails are failing | HackAPrompt CEO"
source: "[[guests/sander-schulhoff-20|Sander Schulhoff 2.0]]"
---
AI guardrails, which are often LLMs trained to detect malicious inputs and outputs, do not provide meaningful security. The [[concepts/adversarial-robustness|adversarial robustness]] of a system is nearly impossible to measure accurately due to the near-infinite attack space, and determined attackers can easily bypass these defenses. Claims of catching "99% of attacks" are statistically meaningless and create a dangerous false sense of security.
---
type: insight
title: Separating malicious intent across multiple requests bypasses many defenses
concepts:
  - "jailbreaking"
  - "ai-security"
source_guest: "Sander Schulhoff"
source_episode: "Why securing AI is harder than anyone expected and guardrails are failing | HackAPrompt CEO"
source: "[[guests/sander-schulhoff-20|Sander Schulhoff 2.0]]"
---
A common attack pattern is to break a malicious objective into smaller, seemingly benign steps across multiple AI interactions. For example, first ask an AI to research a website's backend system, then in a new session ask how to hack that specific system. This [[concepts/jailbreaking|jailbreaking]] technique bypasses defenses that flag single, overtly malicious prompts.
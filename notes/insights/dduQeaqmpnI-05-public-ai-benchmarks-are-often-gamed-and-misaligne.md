---
type: insight
title: Public AI benchmarks are often gamed and misaligned with real-world performance
concepts:
  - "ai-benchmarks"
  - "ai-evaluation"
  - "incentive-misalignment"
source_guest: Edwin Chen
source_episode: The $1B Al company training ChatGPT, Claude & Gemini on the path to responsible AGI | Edwin Chen
source: "[[guests/edwin-chen|Edwin Chen]]"
---
Popular [[concepts/ai-benchmarks|AI benchmarks]] and leaderboards like LLM Arena are flawed because they reward superficial traits like flashy formatting and length over accuracy and helpfulness. This creates [[concepts/incentive-misalignment|incentive misalignment]], pushing labs to optimize for leaderboard scores rather than real-world utility, similar to optimizing for tabloid appeal instead of substantive truth.
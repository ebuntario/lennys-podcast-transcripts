---
type: insight
title: Validate your LLM judge against human judgment
concepts:
  - "ai-evals"
  - "validation"
  - "measurement"
source_guest: Hamel Husain
source_episode: Why AI evals are the hottest new skill for product builders | Hamel Husain & Shreya Shankar
source: "[[guests/hamel-husain-and-shreya-shankar|Hamel Husain & Shreya Shankar]]"
---
Before deploying an LLM judge, measure its agreement with human-labeled data using a confusion matrix. Don't rely solely on overall agreement percentage, as it can be misleading for rare errors. Iterate on the judge's prompt to minimize specific misalignments (e.g., false positives/negatives) to build trust in your [[concepts/ai-evals|evaluation]] system.
---
type: insight
title: Validate your LLM judge against human-labeled data
concepts:
  - "llm-judge"
  - "model-validation"
  - "eval-reliability"
source_guest: Shreya Shankar
source_episode: Why AI evals are the hottest new skill for product builders | Hamel Husain & Shreya Shankar
source: "[[guests/hamelshreya|Hamel+Shreya]]"
---
Before deploying an [[concepts/llm-judge|LLM judge]], measure its agreement with your human-generated axial codes. Don't just look at overall agreement percentage; analyze a confusion matrix to see where disagreements occur. Iterate on the judge's prompt to reduce specific types of misalignment, ensuring the automated eval is trustworthy.